{
 "cells": [
  {
   "cell_type": "markdown",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "71685cce-887b-4711-a484-8668e4ff9e94"
   },
   "outputs": [],
   "source": [
    "# Causal Structures\r\n",
    "Using Halerium Causal Structures\r\n",
    "\r\n",
    "Author: {{ cookiecutter.author_name }}\r\n",
    "Created: {{ cookiecutter.timestamp }}"
   ]
  },
  {
   "cell_type": "markdown",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "48d3ce39-1124-4e2d-ba14-180bb3b8b4b4"
   },
   "outputs": [],
   "source": [
    "## How to use the notebook\r\n",
    "\r\n",
    "The following cells:\r\n",
    "- specify objective, variables, and variable types,\r\n",
    "- read dataset,\r\n",
    "- set up the causal structure,\r\n",
    "- present results from the tests,\r\n",
    "\r\n",
    "By default, the notebook is set up to run with an example (wine quality). To see how it works, run the notebook without changing the code.\r\n",
    "\r\n",
    "For your project, adjust the code in the linked cells with your objectives, variables, dataset etc. and then execute all cells in order.\r\n",
    "\r\n",
    "Please refer to causal_structure.board for detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "9ad815fd-d7fb-470f-8e14-48b52d6e911f"
   },
   "outputs": [],
   "source": [
    "# Link to project experiments folder hypothesis_experiment_learnings.board (refresh and hit enter on this line to see the link)"
   ]
  },
  {
   "cell_type": "markdown",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "72392473-740a-4101-9bb2-d4012d5b37ed"
   },
   "outputs": [],
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "67f4c909-5bbc-4e07-adb4-3e2b1389bfb8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import halerium.core as hal\r\n",
    "\r\n",
    "import itertools\r\n",
    "from itertools import chain, combinations\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "import networkx as nx\r\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "861e898e-57b1-4a2c-b7fd-e04b74efcebd"
   },
   "outputs": [],
   "source": [
    "### Project"
   ]
  },
  {
   "cell_type": "code",
   "connections": [
    {
     "endLine": 1,
     "id": "0a038ced-647c-4248-97fb-7acf81847180",
     "startLine": 1
    }
   ],
   "execution_count": 0,
   "metadata": {
    "id": "de131261-2e2a-4ecd-9a82-70e50470d059"
   },
   "outputs": [],
   "source": [
    "experiment_name = '{{cookiecutter.use_case_name}}'  # please provide a name for the hypothesis testing experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "42890df8-2d60-40f8-afe6-4bef8f21abc7"
   },
   "outputs": [],
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "connections": [
    {
     "endLine": 3,
     "id": "0125f07e-fb0d-47e8-90ff-fd5b9771bece",
     "startLine": 1
    }
   ],
   "execution_count": 0,
   "metadata": {
    "id": "47e4d02c-95e0-48ce-944c-50e34436e888"
   },
   "outputs": [],
   "source": [
    "time_series = False\n",
    "test_size = 0.25\n",
    "path = '{{cookiecutter.data_path}}' # Specify the path of the data\n",
    "\n",
    "if path =='default example':\n",
    "    path = 'https://raw.githubusercontent.com/erium/halerium-example-data/main/hypothesis_testing/WineQT.csv'\n",
    "\n",
    "if time_series:\n",
    "    df = pd.read_csv(path, parse_dates=['date'])\n",
    "else:\n",
    "    df = pd.read_csv(path, sep=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "3448e8b2-0966-47d3-8c49-dacda51954af"
   },
   "outputs": [],
   "source": [
    "## Manual Modelling\r\n",
    "Manually specify the dependencies in the causal structure"
   ]
  },
  {
   "cell_type": "code",
   "connections": [
    {
     "endLine": 4,
     "id": "aa8af7c8-f96c-4333-9da1-3769def1afe1",
     "startLine": 2
    }
   ],
   "execution_count": 0,
   "metadata": {
    "id": "e75a5978-36bd-4e72-b6ec-efa4a8c70625"
   },
   "outputs": [],
   "source": [
    "# Directed dependencies\r\n",
    "dependencies = [['fixed acidity', 'pH'], ['volatile acidity', 'pH']]\r\n",
    "features_input = ['fixed acidity', 'volatile acidity']\r\n",
    "features_output = ['pH']"
   ]
  },
  {
   "cell_type": "code",
   "connections": [
    {
     "endLine": 19,
     "id": "66b970c9-85fd-49f7-b800-f2e9a02fb0c0",
     "path": "/Personal/zihan/hypothesis_testing/causal_inference/causal_structure.ipynb",
     "startLine": 19
    }
   ],
   "execution_count": 0,
   "metadata": {
    "id": "f86a59a7-57d3-4d51-8725-e6b6b02a8ebb"
   },
   "outputs": [],
   "source": [
    "features = list(set([item for sublist in dependencies for item in sublist]))\n",
    "data = df[features]\n",
    "train, test = train_test_split(data, test_size = test_size)\n",
    "\n",
    "causal_structure = hal.CausalStructure(dependencies)\n",
    "causal_structure.train(train)\n",
    "test_input = test[features_input]\n",
    "test_output = test[features_output]\n",
    "test_input.reset_index(inplace=True)\n",
    "test_output.reset_index(inplace=True)\n",
    "\n",
    "influences = []\n",
    "for feature in features:\n",
    "    influence = causal_structure.evaluate_objective(hal.InfluenceEstimator, target=feature)\n",
    "    influences.append([feature, influence])\n",
    "evaluation = causal_structure.evaluate_objective(hal.Evaluator, data=test,\n",
    "                                    inputs=features_input, metric=\"r2\")\n",
    "prediction_mean, prediction_std = causal_structure.predict(data=test_input, return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "f3520164-4dcf-4e7b-a3db-face6ec5a8c8"
   },
   "outputs": [],
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "connections": [
    {
     "endLine": 39,
     "id": "4cfc52f6-8ef5-4434-9d64-16505d9ed731",
     "startLine": 39
    }
   ],
   "execution_count": 0,
   "metadata": {
    "id": "f05cc776-7651-4018-b008-389e8bc0c98c"
   },
   "outputs": [],
   "source": [
    "for feature_out in features_output:\n",
    "    print(\"Output Feature:\", feature_out)\n",
    "    columns = list(prediction_mean.columns) + [feature + ' std' for feature in prediction_mean]\n",
    "    prediction = pd.concat([prediction_mean, prediction_std], axis=1)\n",
    "    prediction.columns = columns\n",
    "    print(\"r2:\", evaluation[feature_out])\n",
    "\n",
    "    for feature_in in features_input:\n",
    "        prediction.sort_values(by=[feature_in], inplace=True)\n",
    "        for feature_out in features_output:\n",
    "            prediction_mean = prediction[features]\n",
    "            prediction_std = prediction[[feature + ' std' for feature in features]]\n",
    "            plt.plot(prediction_mean[feature_in], prediction_mean[feature_out], color=\"red\", label='Predicted data points')\n",
    "            plt.fill_between(prediction_mean[feature_in],\n",
    "                 (prediction_mean - prediction_std)[feature_out],\n",
    "                 (prediction_mean + prediction_std)[feature_out],\n",
    "                 color=\"red\", alpha=0.5)\n",
    "            plt.scatter(test[feature_in], test[feature_out], label='True data points')\n",
    "            plt.xlabel(feature_in)\n",
    "            plt.ylabel(feature_out)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    # Building and displaying the Directed Graph\n",
    "    G = nx.MultiDiGraph()\n",
    "    for feature in features:\n",
    "        G.add_node(feature)\n",
    "    G.add_edges_from(dependencies)\n",
    "\n",
    "    color_map = []\n",
    "    for node in G:\n",
    "        if node in features_output:\n",
    "            color_map.append('red')\n",
    "        else: \n",
    "            color_map.append('green')  \n",
    "\n",
    "    print('Causal Structure')\n",
    "    nx.draw(G, node_color=color_map, with_labels = True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "19110139-8188-4290-9f57-081c992fb0d9"
   },
   "outputs": [],
   "source": [
    "## Automatic Modelling\r\n",
    "Generate all possible DAGs\r\n",
    "This becomes computationally slow with > 3 features"
   ]
  },
  {
   "cell_type": "code",
   "connections": [
    {
     "endLine": 3,
     "id": "aa8af7c8-f96c-4333-9da1-3769def1afe1",
     "startLine": 1
    }
   ],
   "execution_count": 0,
   "metadata": {
    "id": "7995d73a-31bd-489c-b280-e846d863b2df"
   },
   "outputs": [],
   "source": [
    "features = ['fixed acidity', 'volatile acidity', 'pH']\r\n",
    "features_input = ['fixed acidity', 'volatile acidity']\r\n",
    "features_output = ['pH']"
   ]
  },
  {
   "cell_type": "code",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "8dfab1d9-eb82-41a4-9a54-50125da6ea08"
   },
   "outputs": [],
   "source": [
    "# Generate all possible dependencies\r\n",
    "dependencies = []\r\n",
    "for i in itertools.permutations(features, 2):\r\n",
    "    dependencies.append(list(i))\r\n",
    "print(\"Number of dependencies:\", len(dependencies))"
   ]
  },
  {
   "cell_type": "code",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "54c1c0b7-ff2e-4445-962d-9f8a6f135ce0"
   },
   "outputs": [],
   "source": [
    "# Powerset of sets of dependencies of at least size of number of features\r\n",
    "def powerset(iterable):\r\n",
    "    s = list(iterable)\r\n",
    "    min_set_size = 1\r\n",
    "    max_set_size = len(s)\r\n",
    "    return chain.from_iterable(list(combinations(s, r)) for r in range(min_set_size, max_set_size))\r\n",
    "dependency_powerset = list(powerset(dependencies))\r\n",
    "print(\"Length of dependency powerset:\", len(dependency_powerset))"
   ]
  },
  {
   "cell_type": "code",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "6f5240a2-ac5f-4ea4-a27c-8cd159483a41"
   },
   "outputs": [],
   "source": [
    "dag = []\r\n",
    "for dependency_set in dependency_powerset:\r\n",
    "    try:\r\n",
    "        hal.causal_structure.Dependencies(dependency_set)\r\n",
    "    except:\r\n",
    "        continue\r\n",
    "    else:\r\n",
    "        dependencies = list(dependency_set)\r\n",
    "        all_dependencies = list(set([item for sublist in dependencies for item in sublist]))\r\n",
    "        \r\n",
    "        # If it does not include all features specified\r\n",
    "        if set(all_dependencies) != set(features):\r\n",
    "            continue\r\n",
    "        dag.append(dependency_set)\r\n",
    "print(\"Number of DAGs that include all features:\", len(dag))"
   ]
  },
  {
   "cell_type": "code",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "9766efef-24aa-491e-bd55-26cb0d46dc29"
   },
   "outputs": [],
   "source": [
    "results = []\r\n",
    "for count, dependencies in enumerate(dag):\r\n",
    "    print('Model ' + str(count + 1) + '/' + str(len(dag)))\r\n",
    "\r\n",
    "    data = df[features]\r\n",
    "    train, test = train_test_split(data, test_size = test_size)\r\n",
    "    causal_structure = hal.CausalStructure(dependencies)\r\n",
    "    causal_structure.train(train)\r\n",
    "    test_input = test[features_input]\r\n",
    "    test_output = test[features_output]\r\n",
    "    test_input.reset_index(inplace=True)\r\n",
    "    test_output.reset_index(inplace=True)\r\n",
    "\r\n",
    "    influences = []\r\n",
    "    for feature in features:\r\n",
    "        influence = causal_structure.evaluate_objective(hal.InfluenceEstimator, target=feature)\r\n",
    "        influences.append([feature, influence])\r\n",
    "    evaluation = causal_structure.evaluate_objective(hal.Evaluator, data=test,\r\n",
    "                                     inputs=features_input, metric=\"r2\")\r\n",
    "    prediction_mean, prediction_std = causal_structure.predict(data=test_input, return_std=True)\r\n",
    "    \r\n",
    "    print(\"r2 scores\")\r\n",
    "    for output in features_output:\r\n",
    "        print(output, evaluation[output])\r\n",
    "\r\n",
    "    results.append([dependencies, causal_structure, influences, evaluation, prediction_mean, prediction_std])"
   ]
  },
  {
   "cell_type": "markdown",
   "connections": [],
   "execution_count": 0,
   "metadata": {
    "id": "e460403c-8e26-46a0-91a4-2dc72e930510"
   },
   "outputs": [],
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "connections": [
    {
     "endLine": 41,
     "id": "4cfc52f6-8ef5-4434-9d64-16505d9ed731",
     "startLine": 41
    }
   ],
   "execution_count": 0,
   "metadata": {
    "id": "403932ac-f8ec-4d27-8cc3-24fb17dca54f"
   },
   "outputs": [],
   "source": [
    "for feature_out in features_output:\r\n",
    "    print(\"Output Feature:\", feature_out)\r\n",
    "    best_r2 = max(results, key= lambda x: x[3][feature_out])\r\n",
    "    dependencies, causal_structure, influences, evaluation, prediction_mean, prediction_std = best_r2\r\n",
    "    columns = list(prediction_mean.columns) + [feature + ' std' for feature in prediction_mean]\r\n",
    "    prediction = pd.concat([prediction_mean, prediction_std], axis=1)\r\n",
    "    prediction.columns = columns\r\n",
    "    print(\"r2:\", evaluation[feature_out])\r\n",
    "\r\n",
    "    for feature_in in features_input:\r\n",
    "        prediction.sort_values(by=[feature_in], inplace=True)\r\n",
    "        for feature_out in features_output:\r\n",
    "            prediction_mean = prediction[features]\r\n",
    "            prediction_std = prediction[[feature + ' std' for feature in features]]\r\n",
    "            plt.plot(prediction_mean[feature_in], prediction_mean[feature_out], color=\"red\", label=\"Predicted data points\")\r\n",
    "            plt.fill_between(prediction_mean[feature_in],\r\n",
    "                 (prediction_mean - prediction_std)[feature_out],\r\n",
    "                 (prediction_mean + prediction_std)[feature_out],\r\n",
    "                 color=\"red\", alpha=0.5)\r\n",
    "            plt.scatter(test[feature_in], test[feature_out], label='True data points')\r\n",
    "            plt.xlabel(feature_in)\r\n",
    "            plt.ylabel(feature_out)\r\n",
    "            plt.legend()\r\n",
    "            plt.show()\r\n",
    "\r\n",
    "    # Building and displaying the Directed Graph\r\n",
    "    G = nx.MultiDiGraph()\r\n",
    "    for feature in features:\r\n",
    "        G.add_node(feature)\r\n",
    "    G.add_edges_from(dependencies)\r\n",
    "\r\n",
    "    color_map = []\r\n",
    "    for node in G:\r\n",
    "        if node in features_output:\r\n",
    "            color_map.append('red')\r\n",
    "        else: \r\n",
    "            color_map.append('green')  \r\n",
    "\r\n",
    "    print(\"Best fitting causal structure\")\r\n",
    "    nx.draw(G, node_color=color_map, with_labels = True)\r\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
