{"cells": [{"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "53e148ac-0cba-40d5-b545-1efb0a309fdd"}, "outputs": [], "source": ["# Autoencoder Outlier Detection\r\n", "\r\n", "This is a template notebook for autoencoder outlier detection.\r\n", "\r\n", "Author: {{ cookiecutter.author_name }}\r\n", "Created: {{ cookiecutter.timestamp }}\r\n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "01821558-8e1a-41ff-b0f4-714bd7253817"}, "outputs": [], "source": ["# Link to project experiments folder hypothesis_experiment_learnings.board (refresh and hit enter on this line to see the link)"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "38769bc5-32f5-41b0-ae74-310ac57c757e"}, "outputs": [], "source": ["## How to use the notebook\r\n", "\r\n", "The following cells:\r\n", "- specify objective, variables, and data types,\r\n", "- set up the outlier detection models,\r\n", "- read dataset,\r\n", "- present results from the models.\r\n", "\r\n", "By default, the notebook is set up to run with an example (art daily small noise). To see how it works, run the notebook without changing the code.\r\n", "\r\n", "For your project, adjust the code in the linked cells with your objectives, variables, dataset etc. and then execute all cells in order.\r\n", "\r\n", "Please refer to autoencoder.board for detailed instructions."]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "ae0a49ac-4223-4d14-a501-e9f02d0eb97b"}, "outputs": [], "source": ["# <halerium id=\"b2feb61d-a25b-499e-b3ed-7f5e422f78c8\">\n", "# Link to autoencoder.board\n", "# </halerium id=\"b2feb61d-a25b-499e-b3ed-7f5e422f78c8\">\n"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "b7970e8a-e410-4c45-8612-70fd267a8929"}, "outputs": [], "source": ["## Imports"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "db486137-a6cd-4475-9ffd-e0714082bff2"}, "outputs": [], "source": ["import os\r\n", "import shutil\r\n", "\r\n", "import numpy as np\r\n", "import pandas as pd\r\n", "\r\n", "from tensorflow import keras\r\n", "from tensorflow.keras import layers\r\n", "\r\n", "import seaborn as sns\r\n", "import matplotlib.pyplot as plt\r\n", "\r\n", "from distutils.dir_util import copy_tree\r\n", "\r\n", "from sklearn.preprocessing import StandardScaler\r\n", "\r\n", "from joblib import dump, load"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "f819e62a-7017-4323-9c47-61a6a8df733e"}, "outputs": [], "source": ["### 2. Import the Dataset"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "67f52528-f64e-4d9c-988d-f6767e5a6fc5"}, "outputs": [], "source": ["# <halerium id=\"91b472d3-82da-41e9-96e2-8389b64a2329\">\n", "time_series = True # Specify if the data is time series\n", "path = 'default example' # Specify the path of the data, note that it should be 'clean' without anomalies.\n", "# </halerium id=\"91b472d3-82da-41e9-96e2-8389b64a2329\">\n"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "1aa29d47-7148-4e54-b913-d61eb70550ba"}, "outputs": [], "source": ["Importing the dataset"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "e8d59e09-a671-4c45-ab02-a8417e1bc235"}, "outputs": [], "source": ["if path == 'default example':\r\n", "    path = 'https://raw.githubusercontent.com/erium/halerium-example-data/main/outlier_detection/art_daily_small_noise.csv'\r\n", "\r\n", "if time_series:\r\n", "    df = pd.read_csv(path, parse_dates=['date'], index_col = 'date')\r\n", "else:\r\n", "    df = pd.read_csv(path)\r\n", "\r\n", "num_col = len(df.columns)"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "a323f682-a44f-4ac7-b501-b3cdfb5a2e01"}, "outputs": [], "source": ["Visualising the dataset"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "937d2596-060a-44da-9556-637597a3bb7f"}, "outputs": [], "source": ["df"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "ccc91d45-8fec-48d4-896e-b68ecf6c1931"}, "outputs": [], "source": ["Creating the /out folder for artifacts"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "1250be7a-0578-408d-a2fd-08558abfce06"}, "outputs": [], "source": ["path = './out'\r\n", "isExist = os.path.exists(path)\r\n", "if isExist:\r\n", "  for root, dirs, files in os.walk(path):\r\n", "      for f in files:\r\n", "          os.unlink(os.path.join(root, f))\r\n", "      for d in dirs:\r\n", "          shutil.rmtree(os.path.join(root, d))\r\n", "else:\r\n", "  os.makedirs(path)"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "e6d9c3ff-fff9-4af6-9b93-e3f0d61ef1ed"}, "outputs": [], "source": ["from functions.plot import plot_features\r\n", "\r\n", "plot_features(df, time_series, num_col)"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "63ed0f66-d678-4b91-a208-b383536635ff"}, "outputs": [], "source": ["Normalisation"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "aac239fc-4565-4c87-8f7a-18f0bdf7a723"}, "outputs": [], "source": ["scaler = StandardScaler()\r\n", "scaler.fit(df)\r\n", "df = pd.DataFrame(scaler.transform(df), index = df.index, columns = df.columns)\r\n", "df"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "09e51add-4d5f-420a-adaf-5d9bdf9bbc07"}, "outputs": [], "source": ["### 3. Run the Models\r\n", "The autoencoder model would expect sequences as input. These sequences are groups of data and may be grouped together by a common time period (eg. Samples in a day/week/month)."]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "ef57f692-1da9-4671-86c0-44c70439320a"}, "outputs": [], "source": ["from functions.autoencoder import create_sequences\r\n", "\r\n", "# Use an even factor/multiple of 32\r\n", "# <halerium id=\"81b8c3ec-0282-4d4d-bd42-87dc83b06dd9\">\n", "TIME_STEPS = 32 # In the example dataset, there is one data point every 5 minutes. 288 will be the timestamps in a day.\r\n", "# </halerium id=\"81b8c3ec-0282-4d4d-bd42-87dc83b06dd9\">\n", "\r\n", "# Generated training sequences for use in the model.\r\n", "X_train = create_sequences(df.values, TIME_STEPS)\r\n", "print('(Number of timestamps - time steps, time steps, num features)')\r\n", "X_train.shape"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "d5551051-fce0-46a7-a36b-dd3d133b817f"}, "outputs": [], "source": ["# Convolutional Reconstruction Autoencoder\r\n", "# <halerium id=\"81b8c3ec-0282-4d4d-bd42-87dc83b06dd9\">\n", "model = keras.Sequential(\r\n", "    [\r\n", "        layers.Input(shape=(X_train.shape[1], X_train.shape[2])),\r\n", "        layers.Conv1D(\r\n", "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\r\n", "        ),\r\n", "        layers.Dropout(rate=0.2),\r\n", "        layers.Conv1D(\r\n", "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\r\n", "        ),\r\n", "        layers.Conv1DTranspose(\r\n", "            filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\r\n", "        ),\r\n", "        layers.Dropout(rate=0.2),\r\n", "        layers.Conv1DTranspose(\r\n", "            filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\r\n", "        ),\r\n", "        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\r\n", "    ]\r\n", ")\r\n", "# </halerium id=\"81b8c3ec-0282-4d4d-bd42-87dc83b06dd9\">\n", "\r\n", "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\r\n", "model.summary()"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "97a26f52-9270-480a-98c4-441715f1dacf"}, "outputs": [], "source": ["# LSTM for time series\r\n", "# <halerium id=\"81b8c3ec-0282-4d4d-bd42-87dc83b06dd9\">\n", "if time_series:\r\n", "    lstm_model = keras.Sequential()\r\n", "    lstm_model.add(keras.layers.LSTM(\r\n", "        units = 64,\r\n", "        input_shape=(X_train.shape[1], X_train.shape[2])\r\n", "        ))\r\n", "    lstm_model.add(keras.layers.Dropout(rate=0.2))\r\n", "    lstm_model.add(keras.layers.RepeatVector(n=X_train.shape[1]))\r\n", "\r\n", "    lstm_model.add(keras.layers.LSTM(\r\n", "        units = 64,\r\n", "        return_sequences = True\r\n", "        ))\r\n", "    lstm_model.add(keras.layers.Dropout(rate=0.2))\r\n", "    lstm_model.add(keras.layers.TimeDistributed(keras.layers.Dense(units = X_train.shape[2])))\r\n", "\r\n", "    lstm_model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\r\n", "    lstm_model.summary()\n", "# </halerium id=\"81b8c3ec-0282-4d4d-bd42-87dc83b06dd9\">\n"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "82c01553-4023-47ac-9a32-3e102dafcd3d"}, "outputs": [], "source": ["Train the model"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "85dbe83c-1a7b-446e-bed3-91b2d542988c"}, "outputs": [], "source": ["history = model.fit(\r\n", "    X_train,\r\n", "    X_train,\r\n", "    epochs=50,\r\n", "    batch_size=128,\r\n", "    validation_split=0.1,\r\n", "    shuffle=False, # No assumption that data is independent\r\n", "    callbacks=[\r\n", "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\r\n", "    ],\r\n", ")"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "8d571233-b43d-4f40-8662-1f82607cc8b1"}, "outputs": [], "source": ["if time_series:\r\n", "    lstm_history = lstm_model.fit(\r\n", "        X_train,\r\n", "        X_train,\r\n", "        epochs=50,\r\n", "        batch_size=128,\r\n", "        validation_split=0.1,\r\n", "        shuffle=False, # No assumption that data is independent\r\n", "        callbacks=[\r\n", "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\r\n", "        ],\r\n", "    )"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "ef03de87-d62c-42f1-8611-839171172b36"}, "outputs": [], "source": ["### 4. Get the results"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "c9fe1c96-712a-4d2a-a233-28124f6768b5"}, "outputs": [], "source": ["from functions.plot import plot_train_loss\r\n", "\r\n", "plot_train_loss(history)"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "22426d46-d045-4c5e-989e-1dde8c692c0f"}, "outputs": [], "source": ["if time_series:\r\n", "    plot_train_loss(lstm_history)"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "7660bb83-6c6e-4a08-8f8f-e0a33773982d"}, "outputs": [], "source": ["from functions.plot import plot_mae_loss\r\n", "\r\n", "# Get train MAE loss.\r\n", "# <halerium id=\"93af5872-6fc8-4da2-8179-97cb52aac109\">\n", "X_train_pred, train_mae_loss, threshold = plot_mae_loss(model, X_train, df)\n", "# </halerium id=\"93af5872-6fc8-4da2-8179-97cb52aac109\">\n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "04149914-51f7-4647-98ad-b13a5685463c"}, "outputs": [], "source": ["if time_series:\r\n", "    # Get train MAE loss from LSTM model.\r\n", "# <halerium id=\"93af5872-6fc8-4da2-8179-97cb52aac109\">\n", "    lstm_X_train_pred, lstm_train_mae_loss, lstm_threshold = plot_mae_loss(lstm_model, X_train, df)\n", "# </halerium id=\"93af5872-6fc8-4da2-8179-97cb52aac109\">\n"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "2a273394-75d9-4d0a-919f-8e1a30721461"}, "outputs": [], "source": ["from functions.plot import plot_first_sequence\n", "\n", "# Check how the first sequence is learnt\n", "plot_first_sequence(X_train, X_train_pred, df)"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "99bd40cf-e8db-4c46-95b5-2d474f6bc7b6"}, "outputs": [], "source": ["if time_series:\r\n", "    plot_first_sequence(X_train, lstm_X_train_pred, df)"]}, {"cell_type": "markdown", "execution_count": 0, "metadata": {"id": "853bb230-85f9-4639-88ba-e45a09b0220d"}, "outputs": [], "source": ["### 6. Export and use Streamlit\r\n", "Picking the model with the lower total mae loss"]}, {"cell_type": "code", "execution_count": 0, "metadata": {"id": "53df597e-798f-45e3-9007-4e37b50d9a80"}, "outputs": [], "source": ["from functions.autoencoder import export_model\n", "\n", "if time_series:\n", "    if len(df.columns) > 1:\n", "        sum_mae_loss = sum(sum(train_mae_loss))\n", "        lstm_sum_mae_loss = sum(sum(lstm_train_mae_loss))\n", "    else:\n", "        sum_mae_loss = sum(train_mae_loss)\n", "        lstm_sum_mae_loss = sum(lstm_train_mae_loss)\n", "    print(sum_mae_loss)\n", "    print(lstm_sum_mae_loss)\n", "else:\n", "    print(sum_mae_loss)\n", "\n", "if time_series and lstm_sum_mae_loss < sum_mae_loss:\n", "# <halerium id=\"575b8adf-c236-4cdd-9ba9-de22998864ca\">\n", "    export_model(lstm_model, scaler, TIME_STEPS, lstm_threshold, time_series, df)\n", "# </halerium id=\"575b8adf-c236-4cdd-9ba9-de22998864ca\">\n", "else:\n", "# <halerium id=\"575b8adf-c236-4cdd-9ba9-de22998864ca\">\n", "    export_model(lstm_model, scaler, TIME_STEPS, threshold, time_series, df)\n", "# </halerium id=\"575b8adf-c236-4cdd-9ba9-de22998864ca\">\n"]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 0}